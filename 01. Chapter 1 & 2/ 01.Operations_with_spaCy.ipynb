{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYeIkrpxrAjSeGJtEFws7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/MasteringSpaCy/blob/main/01.%20Chapter%201%20%26%202/%2001.Operations_with_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started with spaCy"
      ],
      "metadata": {
        "id": "A6YxV2PL0Bqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "anToa2GHw1o0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "-UPLOQFvxHWx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "0XZ0lrLyw2iA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp('I own a ginger cat.')\n",
        "options = {\"compact\": False, \"distance\": 100}\n",
        "displacy.render(doc, style='dep', options=options, jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "BSJIvBIYw9bw",
        "outputId": "ae0e3ebf-ec01-401f-d3f2-661691ed7554"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"780a0172ba6f4caea40130954be38b40-0\" class=\"displacy\" width=\"550\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">own</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">ginger</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">cat.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-780a0172ba6f4caea40130954be38b40-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,102.0 140.0,102.0 140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-780a0172ba6f4caea40130954be38b40-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-780a0172ba6f4caea40130954be38b40-0-1\" stroke-width=\"2px\" d=\"M270,152.0 C270,52.0 445.0,52.0 445.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-780a0172ba6f4caea40130954be38b40-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M270,154.0 L262,142.0 278,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-780a0172ba6f4caea40130954be38b40-0-2\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-780a0172ba6f4caea40130954be38b40-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M370,154.0 L362,142.0 378,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-780a0172ba6f4caea40130954be38b40-0-3\" stroke-width=\"2px\" d=\"M170,152.0 C170,2.0 450.0,2.0 450.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-780a0172ba6f4caea40130954be38b40-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M450.0,154.0 L458.0,142.0 442.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"Bill Gates is the CEO of Microsoft.\")\n",
        "\n",
        "# Custom warna entity (optional)\n",
        "colors = {\"PERSON\": \"#f4a261\", \"ORG\": \"#2a9d8f\", \"GPE\": \"#e9c46a\"}  # lo bisa ubah sesuai selera\n",
        "options = {\"colors\": colors}\n",
        "\n",
        "# Visualisasi entity dengan warna\n",
        "displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "RzY9S4s4yAnh",
        "outputId": "3ac9ca40-045e-4474-f8df-ff6d310037a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #f4a261; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bill Gates\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is the CEO of \n",
              "<mark class=\"entity\" style=\"background: #2a9d8f; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Microsoft\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Operations with spaCy\n",
        "\n",
        "You will then learn about the first pipeline component – **Tokenizer**. You'll also learn about an important linguistic concept – **lemmatization** – along with its applications in natural language understanding **bold text** (NLU). Following that, we will cover container classes and spaCy data structures in detail. We will finish the chapter with useful spaCy features that\n",
        "you'll use in everyday NLP development."
      ],
      "metadata": {
        "id": "YvuBDotQ0V-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview of spaCy conventions\n",
        "Every NLP application consists of several steps of processing the text. As you can see in the first chapter, we have always created instances called nlp and doc. But what did we do exactly?\n",
        "When we call nlp on our text, spaCy applies some processing steps. The first step is tokenization to produce a Doc object. The Doc object is then processed further with a tagger, a parser, and an entity recognizer. This way of processing the text is called a language processing pipeline. Each pipeline component returns the processed Doc and then passes it to the next component:"
      ],
      "metadata": {
        "id": "H-XRcHcBDOqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure 2.1](https://raw.githubusercontent.com/farrelrassya/MasteringSpaCy/main/01.%20Chapter%201%20%26%202/Figure%202.1.png)\n"
      ],
      "metadata": {
        "id": "c1RcBGPf0TrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A spaCy pipeline object is created when we load a language model. We load an English\n",
        "model and initialize a pipeline in the following code segment:"
      ],
      "metadata": {
        "id": "hzwQqgMAEXZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #We started by importing spaCy.\n",
        "nlp = spacy.load(\"en_core_web_md\") #spacy.load() returned a Language class instance, nlp.\n",
        "doc = nlp(\"I went there\") #we applied nlp on the sample sentence I went there"
      ],
      "metadata": {
        "id": "msm1Z_gIy_jl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Language class applies all of the preceding pipeline steps to your input sentence behind the scenes. After applying nlp to the sentence, the Doc object contains tokens that are tagged, lemmatized, and marked as entities if the token is an entity (we will go into\n",
        "detail about what are those and how it's done later). Each pipeline component has a well-defined task:"
      ],
      "metadata": {
        "id": "By1dc-6VEpsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 2.2 – Pipeline components and tasks\n",
        "\n",
        "| **NAME**   | **COMPONENT**       | **CREATES**                                            | **DESCRIPTION**                          |\n",
        "|------------|---------------------|--------------------------------------------------------|------------------------------------------|\n",
        "| tokenizer  | Tokenizer           | `Doc`                                                  | Segment text into tokens                 |\n",
        "| tagger     | Tagger              | `Doc[i].tag`                                           | Assign part-of-speech tags               |\n",
        "| parser     | DependencyParser    | `Doc[i].head`, `Doc[i].dep`, `Doc.sents`, `Doc.noun_chunks` | Assign dependency labels          |\n",
        "| ner        | EntityRecognizer    | `Doc.ents`, `Doc.ent_iob`, `Doc[i].ent_type`           | Detect and label named entities          |\n"
      ],
      "metadata": {
        "id": "a7jaSHDh0knK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spaCy language processing pipeline always depends on the statistical model and its capabilities. This is why we always load a language model with spacy.load() as the first step in our code.\n",
        "\n",
        "Each component corresponds to a spaCy class. spaCy classes have self-explanatory\n",
        "names such as Language, Doc, and Vocab. We already used Language and Doc\n",
        "classes – let's see all of the processing pipeline classes and their duties:"
      ],
      "metadata": {
        "id": "moEqipY40qqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing pipeline\n",
        "\n",
        "| **TYPE**          | **DESCRIPTION**                                                                                                                                          |\n",
        "|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Language          | A text-processing pipeline. Usually you'll load this once per process as **nlp** and pass the sentences around your application.                         |\n",
        "| Tokenizer         | Segment text, and create **Doc** objects with the discovered segment boundaries.                                                                         |\n",
        "| Lemmatizer        | Determine the base forms of words.                                                                                                                       |\n",
        "| Morphology        | Assign linguistic features like lemmas, noun case, verb tense etc. based on the word and its part-of-speech tag.                                         |\n",
        "| Tagger            | Annotate part-of-speech tags on **Doc** objects.                                                                                                         |\n",
        "| DependencyParser  | Annotate syntactic dependencies on **Doc** objects.                                                                                                      |\n",
        "| EntityRecognizer  | Annotate named entities, e.g. persons or products, on **Doc** objects.                                                                                   |\n",
        "| TextCategorizer   | Assign categories or labels to **Doc** objects.                                                                                                          |\n",
        "| Matcher           | Match sequences of tokens, based on pattern rules, similar to regular expressions.                                                                       |\n",
        "| PhraseMatcher     | Match sequences of tokens, based on phrases.                                                                                                             |\n",
        "| EntityRuler       | Add entity spans to the **Doc** using token-based rules or exact phrase matches.                                                                         |\n",
        "| Sentencizer       | Implement custom sentence boundary detection logic that doesn't require the dependency parse.                                                            |\n"
      ],
      "metadata": {
        "id": "KFdO7Bio0yc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't be intimated by the number of classes; each class has unique features to help you\n",
        "process your text better.\n",
        "There are more data structures to represent text data and language data. Container\n",
        "classes such as Doc hold information about sentences, words, and the text. There are also\n",
        "container classes other than Doc:"
      ],
      "metadata": {
        "id": "S72OR8850yrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Container objects\n",
        "\n",
        "| **NAME** | **DESCRIPTION**                                                                                                                                               |\n",
        "|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Doc      | A container for accessing linguistic annotations                                                                                                             |\n",
        "| Span     | A slice from a Doc object                                                                                                                                    |\n",
        "| Token    | An individual token – i.e. a word, punctuation symbol, whitespace, etc.                                                                                      |\n",
        "| Lexeme   | An entry in the vocabulary. It's a word type with no context, as opposed to a word token. It therefore has no part-of-speech tag, dependency parse, etc.     |\n"
      ],
      "metadata": {
        "id": "OvdPv0ZT1GqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, spaCy provides helper classes for vectors, language vocabulary, and annotations.\n",
        "We'll see the Vocab class often in this book. Vocab represents a language's vocabulary.\n",
        "Vocab contains all the words of the language model we loaded:"
      ],
      "metadata": {
        "id": "PJEbitfH1HBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other classes\n",
        "\n",
        "| **NAME**      | **DESCRIPTION**                                                                                                                                     |\n",
        "|---------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Vocab         | A lookup table buat vocabulary, ngebolehin lo akses Lexeme objects                                                                                 |\n",
        "| StringStore   | Nge-map string ke hash values dan sebaliknya                                                                                                       |\n",
        "| Vectors       | Class container buat vector data yang di-key pake string                                                                                           |\n",
        "| GoldParse     | Kumpulan annotations buat training                                                                                                                 |\n",
        "| GoldCorpus    | Corpus yang diannotate (format JSON). Nge-manage annotations buat tagging, dependency parsing, sama NER                                           |\n"
      ],
      "metadata": {
        "id": "7CaMUiXQ1YAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spaCy library's backbone data structures are Doc and Vocab. The Doc object\n",
        "abstracts the text by owning the sequence of tokens and all their properties. The Vocab\n",
        "object provides a centralized set of strings and lexical attributes to all the other classes.\n",
        "This way spaCy avoids storing multiple copies of linguistic data:"
      ],
      "metadata": {
        "id": "cGxuWYdD1z4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure 2.6](https://raw.githubusercontent.com/farrelrassya/MasteringSpaCy/main/01.%20Chapter%201%20%26%202/Figure%202.6.png)\n"
      ],
      "metadata": {
        "id": "LG8C1WcW13k4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can divide the objects composing the preceding spaCy architecture into two:\n",
        "containers and processing pipeline components. In this chapter, we'll first learn about two basic components Tokenizer and Lemmatizer, then we'll explore Container objects further. spaCy does all these operations for us behind the scenes, allowing us to concentrate on\n",
        "our own application's development. With this level of abstraction, using spaCy for NLP application development is no coincidence. Let's start with the Tokenizer class and see what it offers for us; then we will explore all the container classes one by one throughout\n",
        "the chapter."
      ],
      "metadata": {
        "id": "yG3G-C-i16cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing tokenization\n",
        "\n",
        "We saw in Figure 2.1 that the first step in a text processing pipeline is tokenization.Tokenization is always the first operation because all the other operations require the tokens.\n",
        "\n",
        "Tokenization simply means splitting the sentence into its tokens. A token is a unit of semantics. You can think of a token as the smallest meaningful part of a piece of text. **Tokens** can be words, numbers, punctuation, currency symbols, and any other meaningful symbols that are the building blocks of a sentence. The following are examples of tokens:\n",
        "\n",
        "1. USA\n",
        "2. N.Y.\n",
        "3. city\n",
        "4. 33\n",
        "5. 3rd\n",
        "6. !\n",
        "7. …\n",
        "8. ?\n",
        "9. 's\n",
        "\n",
        "\n",
        "Input to the spaCy tokenizer is a Unicode text and the result is a Doc object. The following code shows the tokenization process:"
      ],
      "metadata": {
        "id": "D-4b9xVU8guG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\") # loaded the English language model via the en shortcut to create an instance of the nlp Language class.\n",
        "doc = nlp(\"I own a ginger cat.\") # apply the nlp object to the input sentence to create a Doc object\n",
        "print ([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwHf8UCTEZSL",
        "outputId": "a10ae5bb-2b4a-4638-e88f-c28c7a87db65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'own', 'a', 'ginger', 'cat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it, we made the tokenization with just three lines of code. You can visualize the tokenization with indexing as follows:"
      ],
      "metadata": {
        "id": "Jqb0T6Q6-EAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure 2.7](https://raw.githubusercontent.com/farrelrassya/MasteringSpaCy/main/01.%20Chapter%201%20%26%202/Figure%202.7.png)\n"
      ],
      "metadata": {
        "id": "nUSW7vaICz4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the examples suggest, tokenization can indeed be tricky. There are many aspects we should pay attention to: punctuation, whitespaces, numbers, and so on. Splitting from the whitespaces with text.split(\" \") might be tempting and looks like it is working for the example sentence I own a ginger cat."
      ],
      "metadata": {
        "id": "9bLRwmGp-I7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about the sentence \"It's been a crazy week!!!\"? If we make a split(\"\n",
        "\") the resulting tokens would be It's, been, a, crazy, week!!!, which is not what\n",
        "you want. First of all, It's is not one token, it's two tokens: it and 's. week!!! is not a valid token as the punctuation is not split correctly. Moreover, !!! should be tokenized\n",
        "per symbol and should generate three !'s. (This may not look like an important detail, but trust me, it is important for sentiment analysis. We'll cover sentiment analysis in Chapter 8,Text Classification with spaCy.) Let's see what spaCy tokenizer has generated:"
      ],
      "metadata": {
        "id": "WyVvrezr-Ujk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"It's been a crazy week!!!\")\n",
        "print ([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkwUVy7s-Z34",
        "outputId": "46a38417-8332-4d0f-8208-f8446fa8aed2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time the sentence is split as follows:\n",
        "\n",
        "![Figure 2.8](https://raw.githubusercontent.com/farrelrassya/MasteringSpaCy/main/01.%20Chapter%201%20%26%202/Figure%202.8.png)\n"
      ],
      "metadata": {
        "id": "6N92Tpt_-Y_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does spaCy know where to split the sentence? Unlike other parts of the pipeline,\n",
        "the tokenizer doesn't need a statistical model. Tokenization is based on language-specific\n",
        "rules. You can see examples the language specified data here: https://github.com/explosion/spaCy/tree/master/spacy/lang.\n",
        "\n",
        "Tokenizer exceptions define rules for exceptions, such as it's , don't , won't,\n",
        "abbreviations, and so on. If you look at the rules for English: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py, you will see that rules look like {ORTH: \"n't\", LEMMA:\n",
        "\"not\"}, which describes the splitting rule for n't to the tokenizer.\n",
        "\n",
        "The prefixes, suffixes, and infixes mostly describe how to deal with punctuation for\n",
        "example, we split at a period if it is at the end of the sentence, otherwise, most probably\n",
        "it's part of an abbreviation such as N.Y. and we shouldn't touch it. Here, ORTH means the\n",
        "text and LEMMA means the base word form without any inflections. The following example\n",
        "shows you the execution of the spaCy tokenization algorithm:\n"
      ],
      "metadata": {
        "id": "IOOZW1POC_30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure 2.9](https://raw.githubusercontent.com/farrelrassya/MasteringSpaCy/main/01.%20Chapter%201%20%26%202/Figure%202.9.png)"
      ],
      "metadata": {
        "id": "oPFKTsnwEEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization rules depend on the grammatical rules of the individual language.Punctuation rules such as splitting periods, commas, or exclamation marks are more or less similar for many languages; however, some rules are specific to the individual language, such as abbreviation words and apostrophe usage. spaCy supports each language\n",
        "having its own specific rules by allowing hand-coded data and rules, as each language has its own subclass."
      ],
      "metadata": {
        "id": "M2__GnTLEUtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Tip 💡:** spaCy provides non-destructive tokenization, which means that we always will be able to recover the original text from the tokens.  Whitespace and punctuation information is preserved during tokenization, so the input text is preserved as it is.\n"
      ],
      "metadata": {
        "id": "uV7sZrdlEsQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every Language object contains a Tokenizer object. The Tokenizer class is the class that performs the tokenization. You don't often call this class directly when you create a Doc class instance, while Tokenizer class acts behind the scenes. When we want to customize the tokenization, we need to interact with this class. Let's see how it is done."
      ],
      "metadata": {
        "id": "tAo5cYa9E2fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing the tokenizer\n",
        "When we work with a specific domain such as medicine, insurance, or finance, we often come across words, abbreviations, and entities that needs special attention. Most domains that you'll process have characteristic words and phrases that need custom tokenization\n",
        "rules. Here's how to add a special case rule to an existing Tokenizer class instance:"
      ],
      "metadata": {
        "id": "kl3An4vfE5hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"lemme that\")\n",
        "print([w.text for w in doc])\n",
        "special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}]\n",
        "nlp.tokenizer.add_special_case(\"lemme\", special_case)\n",
        "print([w.text for w in nlp(\"lemme that\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idAdTdbn88sW",
        "outputId": "13c81714-a47c-4b56-8cb8-2c7a98f38195"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lemme', 'that']\n",
            "['lem', 'me', 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pro tip 💡:** Modify the tokenizer by adding new rules only if you really need to. Trust me, you can get quite unexpected results with custom rules. One of the cases where you really need it is when working with Twitter text, which is usually full of hashtags and special symbols. If you have social media text, first feed some sentences into the spaCy NLP pipeline and see how the tokenization works out."
      ],
      "metadata": {
        "id": "kWm7rq7DFUlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging the tokenizer\n",
        "The spaCy library has a tool for debugging: nlp.tokenizer.explain(sentence). It returns (tokenizer rule/pattern, token) tuples to help us understand what happened exactly during the tokenization. Let's see an example:"
      ],
      "metadata": {
        "id": "j2SUvOmBFQhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "text = \"Let's go!\"\n",
        "doc = nlp(text)\n",
        "tok_exp = nlp.tokenizer.explain(text)\n",
        "for t in tok_exp:\n",
        "    print(t[1], \"\\t\", t[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZK8jJgsFB7n",
        "outputId": "07444ea5-3598-4925-c467-f7ab397b1924"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let \t SPECIAL-1\n",
            "'s \t SPECIAL-2\n",
            "go \t TOKEN\n",
            "! \t SUFFIX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preceding code, we imported spacy and created a Language class instance, nlp, as usual. Then we created a Doc class instance with the sentence Let's go!. After that, we asked the Tokenizer class instance, tokenizer, of nlp for an explanation of the tokenization of this sentence. nlp.tokenizer.explain() explained the rules that the tokenizer used one by one. After splitting a sentence into its tokens, it's time to split a text into its sentences."
      ],
      "metadata": {
        "id": "HGDOzLc6FaQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence segmentation\n",
        "We saw that breaking a sentence into its tokens is not a straightforward task at all. How about breaking a text into sentences? It's indeed a bit more complicated to mark where a sentence starts and ends due to the same reasons of punctuation, abbreviations, and so on.\n",
        "A Doc object's sentences are available via the doc.sents property"
      ],
      "metadata": {
        "id": "QPipQbwYFhqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkeMhr5gFdM-",
        "outputId": "ae20fced-9224-4518-a61b-f90483ccccb6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I flied to N.Y yesterday.\n",
            "It was around 5 pm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determining sentence boundaries is a more complicated task than tokenization. As a\n",
        "result, spaCy uses the dependency parser to perform sentence segmentation. This is a unique feature of spaCy no other library puts such a sophisticated idea into practice. The results are very accurate in general, unless you process text of a very specific genre, such as\n",
        "from the conversation domain, or social media text."
      ],
      "metadata": {
        "id": "1jJwlzqPFoLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we know how to segment a text into sentences and tokenize the sentences. We're ready to process the tokens one by one. Let's start with lemmatization, a commonly used operation in semantics including sentiment analysis."
      ],
      "metadata": {
        "id": "xCSNeSvMGPYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding lemmatization\n",
        "A lemma is the base form of a token. You can think of a lemma as the form in which the token appears in a dictionary. For instance, the lemma of eating is eat; the lemma of eats is eat; ate similarly maps to eat. Lemmatization is the process of reducing the word forms to their lemmas. The following code is a quick example of how to do lemmatization with\n",
        "spaCy:"
      ],
      "metadata": {
        "id": "5aFtOfMTGR_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"I went there for working and worked for 3 years.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OhLVcQQFqYs",
        "outputId": "e45bc201-1354-4d98-fd14-e6b0ec064476"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I\n",
            "went go\n",
            "there there\n",
            "for for\n",
            "working work\n",
            "and and\n",
            "worked work\n",
            "for for\n",
            "3 3\n",
            "years year\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now, you should be familiar with what the first three lines of the code do. Recall that we import the spacy library, load an English model using spacy.load, create a pipeline, and apply the pipeline to the preceding sentence to get a Doc object. Here we iterated over tokens to get their text and lemmas.\n",
        "In the first line you see –PRON-, which doesn't look like a real token. This is a pronoun lemma, a special token for lemmas of personal pronouns. This is an exception for semantic purposes: the personal pronouns you, I, me, him, his, and so on look different,\n",
        "but in terms of meaning, they're in the same group. spaCy offers this trick for the pronoun lemmas. No worries if all of this sounds too abstract – let's see lemmatization in action with a real-\n",
        "world example."
      ],
      "metadata": {
        "id": "PzRJFvVSGqvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization in NLU\n",
        "Lemmatization is an important step in NLU. We'll go over an example in this subsection. Suppose that you design an NLP pipeline for a ticket booking system. Your application processes a customer's sentence, extracts necessary information from it, and then passes it\n",
        "to the booking API. The NLP pipeline wants to extract the form of the travel (a flight, bus, or train), the destination city, and the date. The first thing the application needs to verify is the means\n",
        "of travel:"
      ],
      "metadata": {
        "id": "lQMmkUkoGxbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. fly – flight – airway – airplane - plane\n",
        "\n",
        "2. bus\n",
        "\n",
        "3. railway – train"
      ],
      "metadata": {
        "id": "-c1GpzUTG3x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have this list of keywords and we want to recognize the means of travel by searching\n",
        "the tokens in the keywords list. The most compact way of doing this search is by looking\n",
        "up the token's lemma. Consider the following customer sentences:"
      ],
      "metadata": {
        "id": "3hGPPTPQG7_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. List me all flights to Atlanta.\n",
        "2. I need a flight to NY.\n",
        "3. I flew to Atlanta yesterday evening and forgot my baggage."
      ],
      "metadata": {
        "id": "nZotW3ndM7e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we don't need to include all word forms of the verb fly (fly, flying, flies, flew, and\n",
        "flown) in the keywords list and similar for the word flight; we reduced all possible\n",
        "variants to the base forms – fly and flight. Don't think of English only; languages such as\n",
        "Spanish, German, and Finnish have many word forms from a single lemma as well.\n",
        "Lemmatization also comes in handy when we want to recognize the destination city. There\n",
        "are many nicknames available for global cities and the booking API can process only the\n",
        "official names. The default tokenizer and lemmatizer won't know the difference between\n",
        "the official name and the nickname. In this case, you can add special rules, as we saw in\n",
        "the Introducing tokenization section. The following code plays a small trick:"
      ],
      "metadata": {
        "id": "KwOj40toNAYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Angeltown\"}]], {\"LEMMA\": \"Los Angeles\"})\n",
        "doc = nlp(\"I am flying to Angeltown\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1348hOvAGaim",
        "outputId": "f88c0d43-90dd-486b-e3e9-ada8a66a111a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I\n",
            "am be\n",
            "flying fly\n",
            "to to\n",
            "Angeltown Los Angeles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined a special case for the word Angeltown by replacing its lemma with the\n",
        "official name Los Angeles. Then we added this special case to the AttributeRuler\n",
        "component. When we print the token lemmas, we see that Angeltown maps to Los\n",
        "Angeles as we wished."
      ],
      "metadata": {
        "id": "dB0EYEtBNIQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the difference between lemmatization\n",
        "and stemming\n",
        "A lemma is the base form of a word and is always a member of the language's vocabulary.\n",
        "The stem does not have to be a valid word at all. For instance, the lemma of improvement\n",
        "is improvement, but the stem is improv. You can think of the stem as the smallest part of\n",
        "the word that carries the meaning. Compare the following examples:"
      ],
      "metadata": {
        "id": "Ex8T6ANENKHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Word         | Lemma       |\n",
        "|--------------|-------------|\n",
        "| university   | university  |\n",
        "| universe     | universe    |\n",
        "| universal    | universal   |\n",
        "| universities | university  |\n",
        "| universes    | universe    |\n",
        "| improvement  | improvement |\n",
        "| improvements | improvements|\n",
        "| improves     | improve     |"
      ],
      "metadata": {
        "id": "NUMcQsnnNU-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preceding word-lemma examples show how lemma is calculated by following the\n",
        "grammatical rules of the language. Here, the lemma of a plural form is the singular form,\n",
        "and the lemma of a third-person verb is the base form of the verb. Let's compare them to\n",
        "the following examples of word-stem pairs:\n",
        "\n",
        "| Word         | Stem    |\n",
        "|--------------|---------|\n",
        "| university   | univers |\n",
        "| universe     | univer  |\n",
        "| universal    | univers |\n",
        "| universities | universi|\n",
        "| universes    | univers |\n",
        "| improvement  | improv  |\n",
        "| improvements | improv  |\n",
        "| improves     | improv  |\n"
      ],
      "metadata": {
        "id": "lotimNu2NYAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first and the most important point to notice in the preceding examples is that the stem does not have to be a valid word in the language. The second point is that many words can map to the same stem. Also, words from different grammatical categories can map to the same stem; here for instance, the noun improvement and the verb improves both map to improv.\n",
        "\n",
        "Though stems are not valid words, they still carry meaning. That's why stemming is commonly used in NLU applications.\n",
        "\n",
        "Stemming algorithms don't know anything about the grammar of the language. This class of algorithms works rather by trimming some common suffixes and prefixes from the beginning or end of the word.\n",
        "\n",
        "Stemming algorithms are rough, they cut the word from head and tail. There are\n",
        "several stemming algorithms available for English, including Porter and Lancaster.\n",
        "You can play with different stemming algorithms on NLTK's demo page at\n",
        "http://text-processing.com/demo/stem/.\n",
        "\n",
        "Lemmatization, on the other hand, takes the morphological analysis of the words into consideration. To do so, it is important to obtain the dictionaries for the algorithm to look through in order to link the form back to its lemma. spaCy provides lemmatization via dictionary lookup and each language has its own dictionary."
      ],
      "metadata": {
        "id": "B_oDOGY1NcgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Tip 💡:** Both stemming and lemmatization have their own advantages. Stemming gives very good results if you apply only statistical algorithms to the text, without further semantic processing such as pattern lookup, entity extraction, coreference resolution, and so on. Also stemming can trim a big corpus to a more moderate size and give you a compact representation. If you also use linguistic features in your pipeline or make a keyword search, include lemmatization. Lemmatization algorithms are accurate but come with a cost in terms of computation."
      ],
      "metadata": {
        "id": "VjqzE_ZAONq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy container objects\n",
        "At the beginning of this chapter, we saw a list of container objects including **Doc**, **Token**, **Span**, and **Lexeme**. We already used Token and Doc in our code. In this subsection, we'll see the properties of the container objects in detail.\n",
        "\n",
        "Using container objects, we can access the linguistic properties that spaCy assigns to the text. A container object is a logical representation of the text units such as a document, a token, or a slice of the document. Container objects in spaCy follow the natural structure of the text: a document is composed of sentences and sentences are composed of tokens.\n",
        "\n",
        "We most widely use Doc, Token, and Span objects in development, which represent\n",
        "a document, a single token, and a phrase, respectively. A container can contain other containers, for instance a document contains tokens and spans.\n",
        "\n",
        "Let's explore each class and its useful properties one by one."
      ],
      "metadata": {
        "id": "ogPy3_V6OA_b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2RiCsmzo2zo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doc\n",
        "We created Doc objects in our code to represent the text, so you might have already figured out that Doc represents a text. We already know how to create a Doc object:"
      ],
      "metadata": {
        "id": "kix4rdvxQuiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I like cats.\")"
      ],
      "metadata": {
        "id": "G7gU2RRqNImk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc.text returns a Unicode representation of the document text:"
      ],
      "metadata": {
        "id": "ySbDBbJTQ2iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-hpojkA3Q0d8",
        "outputId": "47e2e059-2643-499b-aa5a-f5a081f9cf2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I like cats.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The building block of a Doc object is Token, hence when you iterate a Doc you get Token objects as items:"
      ],
      "metadata": {
        "id": "D1IvlsOAQ6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHXs5Q0Q3-K",
        "outputId": "9b5b0b97-ffdf-4ecd-cf7e-b9f294fb8f94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "like\n",
            "cats\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same logic applies to indexing:"
      ],
      "metadata": {
        "id": "YXuKdBgWRAcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3X-76ZBQ9CQ",
        "outputId": "5cf2d2ab-8202-437d-981c-3b88ec92c73a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "like"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The length of a Doc is the number of tokens it includes:"
      ],
      "metadata": {
        "id": "pR8EYXAfRDsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfIWKf6nRB-l",
        "outputId": "7d6ee0d4-6072-48b3-968c-503517cba39c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already saw how to get the text's sentences. doc.sents returns an iterator to the list of sentences. Each sentence is a Span object:"
      ],
      "metadata": {
        "id": "Vn-XryTDRHCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a sentence. This is the second sentence\")\n",
        "doc.sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69I7TUT_RFPZ",
        "outputId": "535e0f14-b6fa-4d6c-9bf7-7ab099e28806"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator at 0x7d72f2b8d1b0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = list(doc.sents)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COZnsk3gRJ3U",
        "outputId": "c443a47d-0f21-405d-d7fa-d92dc0019b40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is a sentence., This is the second sentence]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc.ents gives named entities of the text. The result is a list of Span objects. We'll see named entities in detail later – for now, think of them as proper nouns:"
      ],
      "metadata": {
        "id": "JmrDTRLXRRMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I flied to New York with Ashley.\")\n",
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5PpssZRNF2",
        "outputId": "4a51f37d-eb35-4f17-c581-90db4efd0b21"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(New York, Ashley)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another syntactic property is doc.noun_chunks. It yields the noun phrases found in the text:"
      ],
      "metadata": {
        "id": "4tetT-HTRWNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Sweet brown fox jumped over the fence.\")\n",
        "list(doc.noun_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKyxBqi7RT1K",
        "outputId": "460850a7-eed6-4e5e-c5f6-3321489b8ee6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sweet brown fox, the fence]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc.lang_ returns the language that doc created:"
      ],
      "metadata": {
        "id": "bht4olDnRfB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc.lang_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MLc25dDjRYZP",
        "outputId": "c0abc066-5271-4446-b92e-c5d899a4fe6f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A useful method for serialization is doc.to_json. This is how to convert a Doc object to JSON:"
      ],
      "metadata": {
        "id": "2np85rZvRlax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hi\")\n",
        "json_doc = doc.to_json()\n",
        "{\n",
        "\"text\": \"Hi\",\n",
        "\"ents\": [],\n",
        "\"sents\": [{\"start\": 0, \"end\": 3}],\n",
        "\"tokens\": [{\"id\": 0, \"start\": 0, \"end\": 3, \"pos\": \"INTJ\",\n",
        "\"tag\": \"UH\", \"dep\": \"ROOT\", \"head\": 0}]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLYaCSj0Rgdp",
        "outputId": "7c1b29e5-0850-4d33-ef12-8a48c9c4db34"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Hi',\n",
              " 'ents': [],\n",
              " 'sents': [{'start': 0, 'end': 3}],\n",
              " 'tokens': [{'id': 0,\n",
              "   'start': 0,\n",
              "   'end': 3,\n",
              "   'pos': 'INTJ',\n",
              "   'tag': 'UH',\n",
              "   'dep': 'ROOT',\n",
              "   'head': 0}]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pro tip 💡:** You might have noticed that we call `doc.lang_`, not `doc.lang`. `doc.lang` returns the language ID, whereas `doc.lang_` returns the Unicode string of the language, that is, the name of the language. You can see the same convention with Token features in the following, for instance, `token.lemma_`, `token.tag_`, and `token.pos_`."
      ],
      "metadata": {
        "id": "_cu25ZhNR2pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Doc object has very useful properties with which you can understand a sentence's\n",
        "syntactic properties and use them in your own applications. Let's move on to the Token object and see what it offers."
      ],
      "metadata": {
        "id": "pln97SFqRrV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token\n",
        "A Token object represents a word. Token objects are the building blocks of Doc and Span objects. In this section,We usually don't construct a Token object directly, rather we construct a Doc object then access its tokens:"
      ],
      "metadata": {
        "id": "xaOCIr_yRt8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hello Madam!\")\n",
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEfTvjWRoJq",
        "outputId": "9c7b7e3e-e312-43c5-b79e-fb54e4be23dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.text is similar to doc.text and provides the underlying Unicode string:"
      ],
      "metadata": {
        "id": "FajFCM1hnLfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A9zCBVN-nIBO",
        "outputId": "1da7339f-55f7-4000-db70-51a4c0526c92"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.text_with_ws is a similar property. It provides the text with a trailing\n",
        "whitespace if present in the doc:"
      ],
      "metadata": {
        "id": "qz0bgigPnOqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].text_with_ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VkeSdwr6nNUd",
        "outputId": "13df5a9f-d99c-4c84-aea0-bb386c1357d2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2].text_with_ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z9Pa7-YjnQOI",
        "outputId": "656ffff4-39ce-458b-b0f4-17ffe87972ea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the length of a token is similar to finding the length of a Python string:"
      ],
      "metadata": {
        "id": "u7Ba8idgnT0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf6wniXxnSHv",
        "outputId": "60088cdd-4518-47de-c126-51fe0513afb3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.i gives the index of the token in doc:"
      ],
      "metadata": {
        "id": "8vpLeirUnWwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = doc[2]\n",
        "token.i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZwcDr2UnVdC",
        "outputId": "1f2c41cb-4d92-44af-9d9a-a96a593c6dd1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.idx provides the token's character offset (the character position) in doc:"
      ],
      "metadata": {
        "id": "l7Dh8inNnZbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goDvPVgKnYFx",
        "outputId": "a3488b4c-ccc0-4c4e-a169-6dbb2101aee0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbzLillunamF",
        "outputId": "a8fddaa0-b3b6-43ea-b587-c4b6143c18b4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also access the doc that created the token as follows:"
      ],
      "metadata": {
        "id": "YLQrwSeJneAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = doc[0]\n",
        "token.doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmISrX08nbf5",
        "outputId": "2258537b-eaa1-4525-b95c-3177a2668fcd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello Madam!"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the sentence that the token belongs to is done in a similar way to accessing the doc that created the token:"
      ],
      "metadata": {
        "id": "iKFKqNWjnggR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = doc[1]\n",
        "token.sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1xGTJeInfb-",
        "outputId": "8fac9f47-44d1-4261-8579-c19e58cbc0f7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello Madam!"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.is_sent_start is another useful property; it returns a Boolean indicating\n",
        "whether the token starts a sentence:"
      ],
      "metadata": {
        "id": "rCZuom-ZnjxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"He entered the room. Then he nodded.\")\n",
        "doc[0].is_sent_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajMgvqegniUP",
        "outputId": "ada0141c-ca5a-497d-87a5-dcbcd88003be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].is_sent_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8OjFsW9npJv",
        "outputId": "794e4ceb-11fa-4ed3-f423-81fc32f94732"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[6].is_sent_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ybn3kxansUx",
        "outputId": "1d7a8ce7-482d-4492-b96f-b9a48ef5afce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the basic properties of the Token object that you'll use every day. There is another set of properties that are more related to syntax and semantics. We already saw how to calculate the token lemma in the previous section:"
      ],
      "metadata": {
        "id": "_qRMxWLFn2uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I went there.\")\n",
        "doc[1].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYrRK56qntod",
        "outputId": "f71300cb-4462-4399-c982-14aeddaadb1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You already learned that doc.ents gives the named entities of the document. If you\n",
        "want to learn what sort of entity the token is, use token.ent_type_:"
      ],
      "metadata": {
        "id": "7OYCXWDToVEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"President Trump visited Mexico City.\")\n",
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyIXsPyToKpW",
        "outputId": "2e193ca9-4c72-4336-aa51-4e80521c9661"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Trump, Mexico City)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].ent_type_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eFEnsUlroWuU",
        "outputId": "6578639f-9b43-4979-a10f-da981d9b5510"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PERSON'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3].ent_type_ #mexico is # country, city, state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h-aq3lUQoYb0",
        "outputId": "de0d5ac3-9f6a-4191-e007-dd46bf45d870"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two syntactic features related to POS tagging are token.pos_ and token.tag. We'll learn what they are and how to use them in the next chapter. Another set of syntactic features comes from the dependency parser. These features are\n",
        "dep_, head_, conj_, lefts_, rights_, left_edge_, and right_edge_. We'll\n",
        "cover them in the next chapter as well.\n",
        "\n",
        "\n",
        "> **Tip 💡:** spaCy provides non-destructive tokenization, which means that we always will be able to recover the original text from the tokens.  Whitespace and punctuation information is preserved during tokenization, so the input text is preserved as it is.\n"
      ],
      "metadata": {
        "id": "fJ0aaFZ8oiR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Token object has a rich set of features, enabling us to process the text from head to toe. Let's move on to the Span object and see what it offers for us."
      ],
      "metadata": {
        "id": "LH5HUvM7or12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Span\n",
        "Span objects represent phrases or segments of the text. Technically, a Span has to be a\n",
        "contiguous sequence of tokens. We usually don't initialize Span objects, rather we slice a\n",
        "Doc object:"
      ],
      "metadata": {
        "id": "TBe5ICpUo_v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I know that you have been to USA.\")\n",
        "doc[2:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Zv9OiNoaOg",
        "outputId": "f4d6dd81-a5b1-4d63-f1ef-882b02fecc19"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "that you"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to slice an invalid index will raise an IndexError. Most indexing and slicing rules of Python strings are applicable to Doc slicing as well:"
      ],
      "metadata": {
        "id": "xGlDHVkdpfmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"President Trump visited Mexico City.\")\n",
        "doc[4:] # end index empty means rest of the string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWaIkY4KpbBy",
        "outputId": "177fd0b4-57aa-45a4-8546-be0248169329"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "City."
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6w9otZjpqqv",
        "outputId": "24d31ea6-f240-4a8f-f68e-2b7abde1c3cd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mexico City"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[6:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7NagxqMqAHk",
        "outputId": "7d1639d5-a485-4a1b-b3f5-8bd3fb7e826c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42cmwsRqCUJ",
        "outputId": "63097871-2d78-4ebc-f581-988ea3ab4cce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is one more way to create a Span – we can make a character-level slice of a Doc object with char_span :"
      ],
      "metadata": {
        "id": "4D93tszUqUjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You love Atlanta since you're 20.\")\n",
        "doc.char_span(4, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWouCY7EqM_U",
        "outputId": "6ed12d0f-8ae0-49e8-d79b-0eb7c9c1b512"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "love Atlanta"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The building blocks of a Span object are Token objects. If you iterate over a Span object you get Token objects:"
      ],
      "metadata": {
        "id": "MCU12VZ0qv9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:4]\n",
        "for token in span:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGQzVVOdqe03",
        "outputId": "d55fc4f6-433c-46bb-ada6-3227766ccad1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there\n",
            "after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can think of the Span object as a junior Doc object, indeed it's a view of the Doc object it's created from. Hence most of the features of Doc are applicable to Span as well. For instance, len is identical:"
      ],
      "metadata": {
        "id": "WTblvSVvrKPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hello Madam!\")\n",
        "span = doc[1:2]\n",
        "len(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UGba5iFq7Ak",
        "outputId": "10687a64-7ace-49bc-e844-03fa012f6215"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Span object also supports indexing. The result of slicing a Span object is another Span object:"
      ],
      "metadata": {
        "id": "y5A0llL5vSqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]\n",
        "span"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftNc6kScrP_f",
        "outputId": "14d4afb4-3e9a-47ac-a8b0-b611e0d0a763"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "there after you saw"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subspan = span[1:3]\n",
        "subspan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDbYnw-JvVK8",
        "outputId": "382a7186-605e-4d9b-d477-10bc5e2dd514"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "after you"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "char_spans also works on Span objects. Remember the Span class is a junior Doc class, so we can create character-indexed spans on Span objects as well:"
      ],
      "metadata": {
        "id": "Ugnt29RVvcvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]\n",
        "span0 = span.char_span(15,24) #offsetnya invalid\n",
        "print(span0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo6GdvklvZgg",
        "outputId": "99b882bb-f732-4e5b-8f1b-6c95b580268b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span1 = span.char_span(6, 15)\n",
        "print(span1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlwMveJwPfq",
        "outputId": "0f14df21-9b00-4ea0-f364-fafbda145026"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span3 = span.char_span(0, 5)\n",
        "print(span3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zHUBs6BwldI",
        "outputId": "cc2cbd7f-1710-415c-dade-4de483796d22"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]  # \"there after you saw\"\n",
        "print(\"Original span:\", span.text)\n",
        "\n",
        "# Pake alignment_mode 'expand' biar boundaries otomatis dikit diubah\n",
        "span1 = span.char_span(0, 6, alignment_mode='expand')   # Expected: \"there\"\n",
        "span2 = span.char_span(6, 15, alignment_mode='expand')  # Expected: \"after you\"\n",
        "span3 = span.char_span(15, 19, alignment_mode='expand') # Expected: \" saw\" (tapi kemungkinan bakal jadi \"saw\" kalo spasi gak nyambung)\n",
        "\n",
        "print(\"span1:\", span1.text if span1 is not None else None)\n",
        "print(\"span2:\", span2.text if span2 is not None else None)\n",
        "print(\"span3:\", span3.text if span3 is not None else None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXo_tG2owykS",
        "outputId": "b93da3be-bd8b-472d-d6c7-14f71b5becb8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original span: there after you saw\n",
            "span1: there\n",
            "span2: after you\n",
            "span3: saw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like a Token knows the Doc object it's created from; Span also knows the Doc object it's created from:"
      ],
      "metadata": {
        "id": "jgRcje-WvtwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]\n",
        "span.doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypIHg9WPvgrl",
        "outputId": "19780635-929c-4cff-c99d-c8329548b74c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "You went there after you saw me"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span.sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaBuuq3GvwT3",
        "outputId": "b7c8a97b-e52d-42a9-872c-69d4c3fe0865"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "You went there after you saw me"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also locate the Span in the original Doc:"
      ],
      "metadata": {
        "id": "p5tmwCJMxfbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]\n",
        "span.start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGMoWdg2wKRZ",
        "outputId": "85927d72-4179-4dca-fbcb-cd31a40790f8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span.end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ldsqhmxg3g",
        "outputId": "62e925b8-6ff2-4b89-c2b4-f9e8e4ea8fb8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span.start_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxTwG4TixiSr",
        "outputId": "a49b3409-345e-42d6-c08c-7eb3071de093"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span.end_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlaCxwklxlm-",
        "outputId": "5912ce07-5b01-4221-a8be-78b30b805840"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "span.start is the index of the first token of the Span and span.start_char is the\n",
        "start offset of the Span at character level.\n",
        "If you want a brand-new Doc object, you can call span.as_doc(). It copies the data\n",
        "into a new Doc object:"
      ],
      "metadata": {
        "id": "vc1zI55fxoYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You went there after you saw me\")\n",
        "span = doc[2:6]\n",
        "type(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_1ek-Y8xmtX",
        "outputId": "e501bc73-ebc3-490a-ed8a-a906bbfac853"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_doc = span.as_doc()\n",
        "type(small_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu5E5IRLxtEq",
        "outputId": "c1bbfb63-8bd9-4e72-8c77-6a6ad50ac115"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "span.ents, span.sent, span.text, and span.text_wth_ws are similar to their\n",
        "corresponding Doc and Token methods.\n",
        "Dear readers, we have reached the end of an exhaustive section. We'll now go through a few more features and methods for more detailed text analysis in the next section."
      ],
      "metadata": {
        "id": "AePGmS9rxxyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More spaCy features\n",
        "Most of the NLP development is token and span oriented; that is, it processes tags,\n",
        "dependency relations, tokens themselves, and phrases. Most of the time we eliminate\n",
        "small words and words without much meaning; we process URLs differently, and so on.What we do sometimes depends on the token shape (token is a short word or token looks like an URL string) or more semantical features (such as the token is an article, or the token is a conjunction). In this section, we will see these features of tokens with examples. We'll start with features related to the token shape:"
      ],
      "metadata": {
        "id": "g052QCmL-kWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hello, hi!\")\n",
        "doc[0].lower_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JDlog7Msxu7N",
        "outputId": "48429590-ee7e-4dae-bfba-afe052801e59"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.lower_ returns the token in lowercase. The return value is a Unicode string and this feature is equivalent to token.text.lower(). is_lower and is_upper are similar to their Python string method counterparts, islower() and isupper(). is_lower returns True if all the characters are lowercase, while is_upper does the same with uppercase:"
      ],
      "metadata": {
        "id": "IkAa7S8P-x5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"HELLO, Hello, hello, hEllO\")\n",
        "doc[0].is_upper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcDAV-m0-tgm",
        "outputId": "ecc1d5e9-faf2-4e71-ae70-2e36a561ab24"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].is_lower"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQzBxHEZ-3Xo",
        "outputId": "4fcf50aa-0b4b-46a0-b0e6-f8299844a2a1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].is_upper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvTG_wLI-4oh",
        "outputId": "6e5e4f97-bbcb-4b9a-b44d-e82ebea3fb65"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].is_lower"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP1uV5HJ-5lQ",
        "outputId": "ccac8754-af88-4d8e-fd90-830710c5eb88"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_alpha returns True if all the characters of the token are alphabetic letters. Examples of nonalphabetic characters are numbers, punctuation, and whitespace:"
      ],
      "metadata": {
        "id": "W6_umQaR-84V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Cat and Cat123\")\n",
        "doc[0].is_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbyN63y3-7Ko",
        "outputId": "8451db53-5bef-4877-d596-b6be0696b90b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2].is_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne5NRuyG_BNH",
        "outputId": "c77b2463-777d-4c06-832f-1fa5babc7ab9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_ascii returns True if all the characters of token are ASCII characters."
      ],
      "metadata": {
        "id": "GVbCuP78_E_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hamburg and Göttingen\")\n",
        "doc[0].is_ascii"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZBKoKVS_CP4",
        "outputId": "1618e82d-f2e7-40e6-f3e4-cb748eba00a1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2].is_ascii"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxmd_6zx_Gid",
        "outputId": "a1dc1051-664d-4d1a-9259-ae0957001f35"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_digit returns True if all the characters of the token are numbers:"
      ],
      "metadata": {
        "id": "oGwYyaGg_JOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Cat Cat123 123\")\n",
        "doc[0].is_digit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndawjQN3_Hg4",
        "outputId": "b1ec40a1-85e3-4979-f48d-24b10d3afa93"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].is_digit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcaK2fRM_Qcu",
        "outputId": "7577f9d6-58b8-4590-c57d-12757dbb3e0f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2].is_digit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDldYzXy_RpN",
        "outputId": "bed49c24-86f9-46b0-9799-e1faefaa6aab"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_punct returns True if the token is a punctuation mark:"
      ],
      "metadata": {
        "id": "6HASkifV_T3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"You, him and Sally\")\n",
        "doc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_rFt6iZ_Sqd",
        "outputId": "d8460cff-c7f0-4d76-d6a4-30ba76e46c20"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ","
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].is_punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpF9ym_5_W4F",
        "outputId": "5773f1ca-2148-400c-e409-b52dec10911f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_left_punct and is_right_punct return True if the token is a left punctuation\n",
        "mark or right punctuation mark, respectively. A right punctuation mark can be any mark\n",
        "that closes a left punctuation mark, such as right brackets, > or ». Left punctuation marks\n",
        "are similar, with the left brackets < and « as some examples:"
      ],
      "metadata": {
        "id": "G_AxMpZY_aMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"( [ He said yes. ] )\")\n",
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xceXhhBX_Y5V",
        "outputId": "8b8fa0d7-89a9-4557-8158-aea9a2a97069"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "("
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].is_left_punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnHkFEh6_iJW",
        "outputId": "a985f509-4f5a-4a1d-c8ed-9d4c38dac935"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUzdotUR_jH7",
        "outputId": "33b350a7-457e-4059-dc31-9a860e3ae6c0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "["
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].is_left_punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfS0uJqk_kJI",
        "outputId": "86b3c399-b2d8-4ab5-d041-50ad55f056ac"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5QVSlHQ_lUb",
        "outputId": "bb797479-2afe-4a02-b2db-8d6bc1b74178"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "yes"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[4].is_left_punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_HtB2ff_oxB",
        "outputId": "355d9d44-54ee-4dcd-f6f8-290c9fc6e68d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2EQ8l_i_r-S",
        "outputId": "61250aa6-4b6f-4619-fb7b-1cd1ef5f6069"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[-2].is_right_punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5nCsrYO_vA4",
        "outputId": "382d5001-bf44-4958-84df-2299fdeee8be"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_space returns True if the token is only whitespace characters:"
      ],
      "metadata": {
        "id": "IiA0a39__xcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" \")\n",
        "doc[0]\n",
        "len(doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pCDleHU_wN7",
        "outputId": "b3a0b518-696a-4ec4-f346-05f57fc64603"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].is_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QNwF00Z_62J",
        "outputId": "2e8b0167-4206-4da7-c6f8-95089e6d5df8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"  \")\n",
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRM8nqdGMu4o",
        "outputId": "527a2734-0e29-4ba0-caa2-6cdf1bca2729"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svrPLBHNMwbX",
        "outputId": "adf382dc-d062-47e1-8d8f-6712559003d9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].is_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeN6O30bM2WV",
        "outputId": "8b323e5d-0700-4247-96eb-3a6e5eb13afd"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_bracket returns True for bracket characters:"
      ],
      "metadata": {
        "id": "2QeVTf0VM6qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"( You said [1] and {2} is not applicable.)\")\n",
        "doc[0].is_bracket, doc[-1].is_bracket"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9mXh3chM5O0",
        "outputId": "b3a666b4-3469-4c6b-d2a0-15ba5e89e382"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3].is_bracket, doc[5].is_bracket"
      ],
      "metadata": {
        "id": "xJ-TlvDTM8Qv",
        "outputId": "bfe1ed05-2c5c-42f8-fecd-2d78898ebee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[7].is_bracket, doc[9].is_bracket"
      ],
      "metadata": {
        "id": "kt5gu8AiM9kF",
        "outputId": "6f08f088-9a29-4054-89ea-3c6b94f36e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_quote returns True for quotation marks:"
      ],
      "metadata": {
        "id": "lr_b0c5pNBUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"( You said '1\\\" is not applicable.)\")\n",
        "doc[3]"
      ],
      "metadata": {
        "id": "m6p8C07DM_Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12e3f5e-0d1b-4ee8-f30e-e1f1de7d0f85"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3].is_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXtFAHLLL0OZ",
        "outputId": "9e796716-cb31-44c8-9a4f-289e032efe57"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJOfk79MAZV",
        "outputId": "42fa3de7-2b42-4ef5-8460-3d1789f23be4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\""
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[5].is_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whQvmoyNMB1h",
        "outputId": "beeb78d4-33bd-425f-a629-edbd1739d6d7"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_currency returns True for currency symbols such as $ and € (this method was\n",
        "implemented by myself):"
      ],
      "metadata": {
        "id": "X2mUjTfjMEwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I paid 12$ for the tshirt.\")\n",
        "doc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXNiohG2MDEn",
        "outputId": "caba8d73-71e2-49dc-8db6-20e707f6a0c2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "$"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3].is_currency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wpls1_sMGph",
        "outputId": "44a2196f-dcb2-4254-e764-817eb1049f65"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "like_url, like_num, and like_email are methods about the token shape and\n",
        "return True if the token looks like a URL, a number, or an email, respectively. These\n",
        "methods are very handy when we want to process social media text and scraped web\n",
        "pages:"
      ],
      "metadata": {
        "id": "2HTv6ffCMKuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I emailed you at least 100 times\")\n",
        "doc[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwvwKm9IMIB8",
        "outputId": "0ceef8ae-0d9d-40ec-e8d5-279e6284ee0b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[-2].like_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owd5SVcGMMyD",
        "outputId": "e3bb96f9-188e-45e6-dae2-6e402b6331e6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I emailed you at least hundred times\")\n",
        "doc[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlit1UI0MOCc",
        "outputId": "ef173055-edc5-4d61-ab45-f6b25b3c9641"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hundred"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[-2].like_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4rHPcDCMP9_",
        "outputId": "35796a67-97b5-4d70-a6a8-6c88edf6493b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"My email is duygu@packt.com and you can visit me under https://duygua.github.io any time you want.\")\n",
        "doc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GloQm-foMRom",
        "outputId": "a7907da8-db50-4fbc-c111-e46663add973"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "duygu@packt.com"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[3].like_email"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsPyn2_eMTIu",
        "outputId": "cb6867e8-b6a2-41d3-d651-0cb544579e46"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5AJJya0MX-d",
        "outputId": "ec47af30-69e7-4d5a-b4dc-e85f1e12a25d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "https://duygua.github.io"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[10].like_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZWHofxHMZzs",
        "outputId": "8d7111ba-2582-4dad-ce96-eb4884e071a1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token.shape_ is an unusual feature – there is nothing similar in other NLP libraries. It\n",
        "returns a string that shows a token's orthographic features. Numbers are replaced with d,\n",
        "uppercase letters are replaced with X, and lowercase letters are replaced with x. You can\n",
        "use the result string as a feature in your machine learning algorithms, and token shapes\n",
        "can be correlated to text sentiment:"
      ],
      "metadata": {
        "id": "qW8IVjyUMkmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\n",
        "for token in doc:\n",
        "  print(token.text, token.shape_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOWl2kPPMcO0",
        "outputId": "43cfc317-3404-4687-bfb3-a8cdab34a388"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Girl Xxxx\n",
            "called xxxx\n",
            "Kathy Xxxxx\n",
            "has xxx\n",
            "a x\n",
            "nickname xxxx\n",
            "Cat123 Xxxddd\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_oov and is_stop are semantic features, as opposed to the preceding shape features.\n",
        "is_oov returns True if the token is Out Of Vocabulary (OOV), that is, not in the Doc\n",
        "object's vocabulary. OOV words are unknown words to the language model, and thus also\n",
        "to the processing pipeline components:"
      ],
      "metadata": {
        "id": "a7zjp94jM6g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I visited Jenny at Mynks Resort\")\n",
        "for token in doc:\n",
        "  print(token, token.is_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D85iz_oXMo1W",
        "outputId": "83e5f7f2-2a19-43b4-f976-82e370d63a5e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I False\n",
            "visited False\n",
            "Jenny False\n",
            "at False\n",
            "Mynks True\n",
            "Resort False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is_stop is a feature that is frequently used by machine learning algorithms. Often, we\n",
        "filter words that do not carry much meaning, such as the, a, an, and, just, with, and so on.\n",
        "Such words are called stop words. Each language has their own stop word list, and you can\n",
        "access English stop words here https://github.com/explosion/spaCy/blob/\n",
        "master/spacy/lang/en/stop_words.py:"
      ],
      "metadata": {
        "id": "vA2tig9bNX-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I just want to inform you that I was with the principle.\")\n",
        "for token in doc:\n",
        "  print(token, token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Hau-7dNSe-",
        "outputId": "7660101f-4e6c-409d-a79a-a5cc4df2d1eb"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I True\n",
            "just True\n",
            "want False\n",
            "to True\n",
            "inform False\n",
            "you True\n",
            "that True\n",
            "I True\n",
            "was True\n",
            "with True\n",
            "the True\n",
            "principle False\n",
            ". False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have exhausted the list of spaCy's syntactic, semantic, and orthographic features.\n",
        "Unsurprisingly, many methods focused on the Token object as a token is the syntactic unit\n",
        "of a text."
      ],
      "metadata": {
        "id": "4UVCyT16OAEU"
      }
    }
  ]
}